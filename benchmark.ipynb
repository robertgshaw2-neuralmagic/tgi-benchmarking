{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type llama to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from text_generation_server.models.flash_llama import FlashLlama\n",
    "\n",
    "model_id = \"meta-llama/Llama-2-7b-hf\"\n",
    "model = FlashLlama(model_id=model_id, dtype=torch.bfloat16,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Sep 28 13:30:01 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 520.61.05    Driver Version: 520.61.05    CUDA Version: 11.8     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA A10G         On   | 00000000:00:1E.0 Off |                    0 |\n",
      "|  0%   28C    P0    60W / 300W |  13972MiB / 23028MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from text_generation_server.models.flash_causal_lm import FlashCausalLMBatch\n",
    "from text_generation_server.pb import generate_pb2\n",
    "\n",
    "max_input_length = 256\n",
    "max_batch_size = 32\n",
    "max_prefill_tokens = max_input_length * max_batch_size - 32\n",
    "\n",
    "warmup_requests = []\n",
    "n_tokens = 0\n",
    "while n_tokens < max_prefill_tokens:\n",
    "    warmup_requests.append(\n",
    "        generate_pb2.Request(\n",
    "            id=0,\n",
    "            inputs=\"_text\" * max_input_length,\n",
    "            truncate=min(max_input_length, max_prefill_tokens - n_tokens),\n",
    "            parameters=generate_pb2.NextTokenChooserParameters(\n",
    "                do_sample=False\n",
    "            ),\n",
    "            stopping_parameters=generate_pb2.StoppingCriteriaParameters(\n",
    "                max_new_tokens=2\n",
    "            )\n",
    "        ),\n",
    "    )\n",
    "    \n",
    "    n_tokens += max_input_length\n",
    "\n",
    "warmup_batch = generate_pb2.Batch(id=0, requests=warmup_requests, size=len(warmup_requests))\n",
    "\n",
    "fclm_warmup_batch = FlashCausalLMBatch.from_pb(\n",
    "    pb=warmup_batch,\n",
    "    tokenizer=model.tokenizer,\n",
    "    dtype=model.dtype,\n",
    "    device=model.device,\n",
    ")\n",
    "\n",
    "max_supported_total_tokens = model.warmup(batch=fclm_warmup_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a galaxy far, far away, a long time ago, a young boy named Luke Skywalker was born. Luke was a dreamer, and he dreamed of becoming a Jedi Knight. But Luke’s dreams were put on hold when his home planet of Tatooine was attacked by the evil Empire. Luke’s family was killed, and he was forced to flee to the planet of Dagobah to train with the wise Jedi Master Yoda.\n",
      "Luke’s training was\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "BATCH_SIZE = 1\n",
    "MAX_NEW_TOKENS = 100\n",
    "\n",
    "def make_clm_batch(batch_size=1, max_new_tokens=100):\n",
    "    parameters = generate_pb2.NextTokenChooserParameters(\n",
    "        watermark=False,\n",
    "        temperature=1.0,\n",
    "        repetition_penalty=1.0,\n",
    "        top_k=0,\n",
    "        top_p=1.0,\n",
    "        typical_p=1.0,\n",
    "        do_sample=False\n",
    "    )\n",
    "\n",
    "    stopping_parameters = generate_pb2.StoppingCriteriaParameters(\n",
    "        max_new_tokens=MAX_NEW_TOKENS,\n",
    "        ignore_eos_token=True\n",
    "    )\n",
    "\n",
    "    input_lst = [\n",
    "        \"In a galaxy far, far away\"\n",
    "    ]\n",
    "\n",
    "    requests = [\n",
    "        generate_pb2.Request(\n",
    "            id=idx,\n",
    "            inputs=inputs,\n",
    "            truncate=max_input_length,\n",
    "            parameters=parameters,    \n",
    "            stopping_parameters=stopping_parameters\n",
    "        )\n",
    "        for idx, inputs in enumerate(input_lst * batch_size)\n",
    "    ]\n",
    "\n",
    "    return FlashCausalLMBatch.from_pb(\n",
    "        pb=generate_pb2.Batch(id=0, requests=requests),\n",
    "        tokenizer=model.tokenizer,\n",
    "        dtype=model.dtype,\n",
    "        device=model.device,\n",
    "    )\n",
    "\n",
    "texts = {\n",
    "    idx: request.inputs\n",
    "    for idx, request in enumerate(fclm_batch.requests)\n",
    "}\n",
    "\n",
    "for _ in range(MAX_NEW_TOKENS):\n",
    "    generations, fclm_batch = model.generate_token(fclm_batch)\n",
    "    for idx, gen in enumerate(generations):\n",
    "        texts[idx] += gen.token_text\n",
    "\n",
    "print(texts[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "proto  server\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python3: can't open file '/usr/src/tgi-benchmarking/benchmark.py': [Errno 2] No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!python3 tgi-benchmarking/benchmark.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
